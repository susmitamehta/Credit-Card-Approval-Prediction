{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Project : Credit Card Approval \n###### This Dataset is taken from the below link :\nhttps://www.kaggle.com/datasets/rikdifos/credit-card-approval-prediction\n###### We will be focusing on varaious parts of Data Science Life Cycles Some of them are given below :\n1. Data Analysis\n2. Feature Engineering\n3. Feature Selection\n4. Model Selection\n5. Model Deployment","metadata":{}},{"cell_type":"markdown","source":"Now, We are going to understand the data what does it says about itself.","metadata":{}},{"cell_type":"markdown","source":"###### Import Libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport missingno as msno\nimport datetime\nfrom datetime import timedelta\npd.pandas.set_option('display.max_columns',None)\nsns.set_style('whitegrid')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Reading Data\nindependent_feature = pd.read_csv('../input/credit-card-approval-prediction/application_record.csv')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Let's view the data\nindependent_feature.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Check It's Size\nindependent_feature.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Now, We are going to read the second file what does it contains.\ndependent_feature = pd.read_csv('../input/credit-card-approval-prediction/credit_record.csv')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Let's view the data\ndependent_feature.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Let's check it's shape\ndependent_feature.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, We will be merging the two dataframes to deal with only one dataframe. We will merge by taking intersection of both dataframes based on their 'ID'.","metadata":{}},{"cell_type":"code","source":"data = independent_feature.merge(dependent_feature,how='inner',on=['ID'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Let's check the data\ndata.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Let's check the shape of dataset\ndata.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Cleaning","metadata":{}},{"cell_type":"markdown","source":"Let's Check for below stuff :\n1. Check missing value\n2. Deal with missing value\n3. Check Duplicated value\n4. Drop Unnecceray Columns\n5. Check dtypes of all Columns","metadata":{}},{"cell_type":"code","source":"## Let's find out info about our data\ndata.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.describe()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Let's check is there any null value\ndata.isnull().sum().sort_values(ascending=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Let's visualize null values\ncols = data.columns\nsns.heatmap(data[cols].isnull(),cmap='Blues',yticklabels=False,cbar=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Hence, We get from this heatmap that only Occurance_type have some null values.","metadata":{}},{"cell_type":"code","source":"## Now Let's check if this data have any duplicated data or not\ndata.duplicated().sum()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Hence, Data have not any duplicated values. Now, let's deal with those null values.","metadata":{}},{"cell_type":"code","source":"## Let's dig into OCCUPATION_TYPE column\ndata.OCCUPATION_TYPE.value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Let's fill those null values with 'others'.\ndata['OCCUPATION_TYPE'].fillna('others',inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineering","metadata":{}},{"cell_type":"code","source":"## Let's view our data\ndata.head(3)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Let's dig into CODE_GENDER column\ndata.CODE_GENDER.value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Now, Let's convert F and M into dummies variable\nMale = pd.get_dummies(data['CODE_GENDER'],drop_first=True)\nMale.value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here, 1 represent to Male and 0 represent to Female.","metadata":{}},{"cell_type":"code","source":"## Now, Let's dig into FLAG_OWN_CAR\ndata.FLAG_OWN_CAR.value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Now, Let's convert N and Y into dummies variable\nCar = pd.get_dummies(data['FLAG_OWN_CAR'],drop_first=True)\nCar.value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here, 1 means this person have a car or 0 means this person doesn't own a car.","metadata":{}},{"cell_type":"code","source":"## Now, Let's dig into property column\ndata.FLAG_OWN_REALTY.value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Now, Let's convert it into dummy variable\nProperty = pd.get_dummies(data['FLAG_OWN_REALTY'],drop_first=True)\nProperty.value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here, 1 denotes that this person holds some property and 0 means that this person doesn't have any property.","metadata":{}},{"cell_type":"code","source":"## Let's merge those dummy variables\ndata = pd.concat([data,Property,Car,Male],axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Now, Let's dig about another columns\ndata.NAME_INCOME_TYPE.value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.NAME_EDUCATION_TYPE.value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's replace Secondary / secondary special to Secondary education.","metadata":{}},{"cell_type":"code","source":"## Defining a function\ndef education(x):\n    if x=='Secondary / secondary special':\n        x=x.split(' /')[0]\n    return x\n\ndata['NAME_EDUCATION_TYPE'] = data['NAME_EDUCATION_TYPE'].apply(education)\n       ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.NAME_EDUCATION_TYPE.value_counts() ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.NAME_FAMILY_STATUS.value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's replace Single / not married to Single","metadata":{}},{"cell_type":"code","source":"## Defining a function\ndef family(x):\n    if x=='Single / not married':\n        x=x.split(' /')[0]\n    return x\n\ndata['NAME_FAMILY_STATUS'] = data['NAME_FAMILY_STATUS'].apply(family)\n       ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.NAME_FAMILY_STATUS.value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.NAME_HOUSING_TYPE.value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's replace House / apartment to House","metadata":{}},{"cell_type":"code","source":"## Defining a function\ndef housing(x):\n    if x=='House / apartment':\n        x=x.split(' /')[0]\n    return x\n\ndata['NAME_HOUSING_TYPE'] = data['NAME_HOUSING_TYPE'].apply(housing)\n       ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.NAME_HOUSING_TYPE.value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Let's define some functions for some columns for better understanding of data","metadata":{}},{"cell_type":"code","source":"## This function takes no of days and convert it into their datetime format\ndef Date_of_Birth(total_days):\n    today = datetime.date.today()\n    birthday = (today + timedelta(days=total_days)).strftime('%Y-%m-%d')\n    return birthday                  \n\n## This Function takes value of colunn Days of Employed and convert it into datetime format\ndef Employed_day(total_days):\n    today = datetime.date.today()\n    employed_date = (today + datetime.timedelta(days=total_days)).strftime('%Y-%m-%d')\n    return employed_date\n\n## This function for calculating age\ndef age(days_birth):\n    days_birth = datetime.datetime.strptime(days_birth, '%Y-%m-%d')\n    today = datetime.date.today()\n    Age = today.year - days_birth.year\n    return Age","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, We will be going to apply those functions to their respective columns.","metadata":{}},{"cell_type":"code","source":"## Applying Functions\n## To get date of birth of each person\ndata['DAYS_BIRTH'] = data['DAYS_BIRTH'].apply(Date_of_Birth)\n\n## To get date of employed\ndata['DAYS_EMPLOYED'] = data['DAYS_EMPLOYED'].apply(Employed_day)\n\n## To find the age of every person\ndata['Age'] = data['DAYS_BIRTH'].apply(age)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Now, Let's view the modified data\ndata.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Now, Let's merge those dummy variables and drop the useless columns.","metadata":{}},{"cell_type":"code","source":"## Let's drop useless columns\ndata = data.drop(['ID','DAYS_BIRTH','MONTHS_BALANCE','FLAG_WORK_PHONE','DAYS_EMPLOYED'],axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# replacing the value of C,X to numeric value \ndata.loc[data['STATUS']=='C','STATUS'] = 6\ndata.loc[data['STATUS']=='X','STATUS'] = 7","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.drop(columns=['CODE_GENDER','FLAG_OWN_CAR','FLAG_OWN_REALTY','OCCUPATION_TYPE'],axis=1,inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Labeling :\nWe have to label the datasets because this dataset doesn't contain any target values. We will be going to mark 1(risky people) or 0 (Not risky people) based on 'STATUS' if someone have more than 60 days of installments then it is risky to give credit card to those people.","metadata":{}},{"cell_type":"code","source":"data.STATUS.value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Let's convert dtypes of status column.\ndata['STATUS'] = data['STATUS'].astype(float)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Define a function for labeling the data\ndef get_label_for_data(x):\n    target = ''\n    if x in (2,3,4,5) :\n       target = 1 #risky\n    else:\n         target = 0  #not risky\n\n    return target","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['target'] = data['STATUS'].apply(get_label_for_data)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.target.value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Visualization","metadata":{}},{"cell_type":"code","source":"## Let's create some plots for target values\nsns.countplot(y=data['target'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###### We conclude that we have imbalanced datasets. So, we have to use over scaling later.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.heatmap(data.corr(),annot=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Let's find out correlation between continuous features\ndata.corr()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Let's visualize FLAG_MOBIL\nsns.countplot(y=data['FLAG_MOBIL'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Hence, All of them have Mobile Phones.","metadata":{}},{"cell_type":"code","source":"## Let's visualize FLAG_EMAIL\nsns.countplot(y=data['FLAG_EMAIL'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.FLAG_EMAIL.value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Let's find out about how many family members have\nplt.figure(figsize=(15,10))\nsns.countplot(x=data['CNT_FAM_MEMBERS'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.barplot(x=data['CNT_FAM_MEMBERS'],y=data['AMT_INCOME_TOTAL'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.figure(figsize=(10,12))\n# e=(0.05,0.02,0,0,0)\n# m=data['NAME_FAMILY_STATUS']=='Married'\n# m=m.sum()\n# s=data['NAME_FAMILY_STATUS']=='Single'\n# s=s.sum()\n# Cv=data['NAME_FAMILY_STATUS']=='Civil marriage'\n# Cv=Cv.sum()\n# sep=data['NAME_FAMILY_STATUS']=='Separated'\n# sep=sep.sum()\n# w=data['NAME_FAMILY_STATUS']=='Widow'\n# w=w.sum()\n# y=np.array([m,s,Cv,sep,w])\n# label=['Married','Single','Civil marriage','Separated','Widow']\n# plt.pie(y,explode=e,labels=label)\n# plt.legend(title=\"Title\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Scaling","metadata":{}},{"cell_type":"code","source":"## Let's see head of data\ndata.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Let's formed a dictionary\nlst = {'Working':1,'Commercial associate':2,'Pensioner':3,'State servant':4,'Student':5}\nlst1 = {'Secondary':1,'Higher education':2,'Incomplete higher':3,'Lower secondary':4,'Academic degree':5}\nlst2 = {'Married':1,'Single':2,'Civil marriage':3,'Separated':4,'Widow':5}\nlst3 = {'House':1,'With parents':2,'Municipal apartment':3,'Rented apartment':4,'Office apartment':5,'Co-op apartment':6}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.replace({'NAME_INCOME_TYPE':lst},inplace=True)\ndata.replace({'NAME_EDUCATION_TYPE':lst1},inplace=True)\ndata.replace({'NAME_FAMILY_STATUS':lst2},inplace=True)\ndata.replace({'NAME_HOUSING_TYPE':lst3},inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.target.value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = data.drop('target',axis=1)\nY = data['target']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Let's check the skewness of data.\ndata.skew()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###### Hence,target value is far from Normal Distrubition. so, we are going to use MinMaxScaler.","metadata":{}},{"cell_type":"code","source":"feature_scale = [feature for feature in data.columns if feature!='target']\n## Importing library \nfrom sklearn.preprocessing import MinMaxScaler\nscaler=MinMaxScaler()\nscaler.fit(X)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler.transform(X)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.concat([data['target'].reset_index(drop=True),\n                    pd.DataFrame(scaler.transform(X), columns=feature_scale)],\n                    axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dealing with Unbalanced Dataset","metadata":{}},{"cell_type":"code","source":"## Performing Over-Sampling\n\n## Importing libraries\nfrom imblearn.combine import SMOTETomek ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"smk = SMOTETomek(random_state=42)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_res,Y_res = smk.fit_resample(X,Y)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_res.shape,Y_res.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import Counter","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('original dataset shape {}'.format(Counter(Y)))\nprint('Resampled dataset shape {}'.format(Counter(Y_res)))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Train-Test split","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train,x_test,y_train,y_test = train_test_split(X_res,Y_res,test_size=0.2,random_state=100)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train.shape,x_test.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Selection","metadata":{}},{"cell_type":"markdown","source":"Let's choose some models then choose which one is performing better based on confusion matrix.","metadata":{}},{"cell_type":"code","source":"## import libraries\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import ConfusionMatrixDisplay,accuracy_score","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_list = [LogisticRegression,RandomForestClassifier,DecisionTreeClassifier,GaussianNB,KNeighborsClassifier]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy =[]\nfor model in model_list :\n    model = model()\n    model.fit(x_train,y_train)\n    y_pred = model.predict(x_test)\n    ConfusionMatrixDisplay.from_predictions(y_test,y_pred)\n    accuracy.append(accuracy_score(y_test,y_pred))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion\n##### From this model we conlude that KNNeigbhours and Decision Trees are the best model that can predict whether  we should approve credit card to a person or not based on their data. ","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}